{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1967f5c6-1f90-40d8-bd46-8759fee96a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install dbldatagen\n",
    "\n",
    "# import dbldatagen as dg\n",
    "# from pyspark.sql import functions as F, types as T\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# def drop_volume(chapter_number: str):\n",
    "#     spark.sql(f\"drop volume if exists workspace.default.chapter_{chapter_number}\")\n",
    "\n",
    "# def create_volume(chapter_number: str):\n",
    "#     spark.sql(f\"\"\"\n",
    "#         create volume if not exists workspace.default.chapter_{chapter_number}\n",
    "#         comment 'Datasets for chapter {chapter_number}'\n",
    "#     \"\"\")\n",
    "\n",
    "# def create_enumerated_files(temp_path: str, table_name: str, file_format: str):\n",
    "#     data_files = [f for f in dbutils.fs.ls(temp_path) if f.name.startswith(\"part-\")]\n",
    "#     for index, file_name in enumerate(data_files, start=1):\n",
    "#         path = temp_path.replace('temp/', '')\n",
    "#         dbutils.fs.mv(file_name.path, f\"{path}{table_name}_0{index}.{file_format}\")\n",
    "\n",
    "# def create_complex_data_files(chapter_number: str, temp_root: str):\n",
    "#     od_path = f\"/Volumes/workspace/default/chapter_{chapter_number}/order_details/parquet/\"\n",
    "#     df = spark.read.parquet(od_path)\n",
    "\n",
    "#     array_products = (\n",
    "#         df.groupBy(\"order_id\")\n",
    "#           .agg(F.collect_list(\"product_id\").alias(\"array_products\"))\n",
    "#     )\n",
    "\n",
    "#     order_details_dict = (\n",
    "#         df.groupBy(\"order_id\")\n",
    "#           .agg(F.collect_list(F.struct(\n",
    "#                 F.col(\"product_id\").alias(\"productid\"),\n",
    "#                 F.col(\"unit_price\").alias(\"unitprice\"),\n",
    "#                 F.col(\"discount\").alias(\"discount\")\n",
    "#           )).alias(\"order_details\"))\n",
    "#     )\n",
    "\n",
    "#     (array_products.repartition(4)\n",
    "#         .write.mode(\"overwrite\").format(\"json\")\n",
    "#         .save(f\"{temp_root}/array_products_by_order/json/\"))\n",
    "\n",
    "#     (order_details_dict.repartition(4)\n",
    "#         .write.mode(\"overwrite\").format(\"json\")\n",
    "#         .save(f\"{temp_root}/order_details_dict/json/\"))\n",
    "\n",
    "# def write_all_formats(df, main_temp_path: str, table_name: str, one_file: bool = False):\n",
    "#     dict_types = { \n",
    "#         \"csv\":     {\"options\": {\"delimiter\": \",\", \"header\": \"true\"}},\n",
    "#         \"parquet\": {\"options\": {}},\n",
    "#         \"json\":    {\"options\": {}},\n",
    "#         \"avro\":    {\"options\": {}},\n",
    "#         \"orc\":     {\"options\": {}}\n",
    "#     }\n",
    "#     for fmt, spec in dict_types.items():\n",
    "#         tmp = f\"{main_temp_path}/{table_name}/{fmt}/\"\n",
    "#         writer_df = df.coalesce(1) if one_file else df.repartition(4)\n",
    "#         (writer_df.write\n",
    "#             .mode(\"overwrite\")\n",
    "#             .options(**spec[\"options\"])\n",
    "#             .format(fmt)\n",
    "#             .save(tmp))\n",
    "#         create_enumerated_files(tmp, table_name, fmt)\n",
    "\n",
    "# def generate_and_write_to_volume(\n",
    "#     chapter_number: str,\n",
    "#     users_n: int = 1000,\n",
    "#     orders_n: int = 500,\n",
    "#     orphan_rate: float = 0.01,\n",
    "#     seed_users: int = 101,\n",
    "#     seed_orders: int = 202,\n",
    "#     seed_details: int = 303,\n",
    "#     seed_lines: int = 404,\n",
    "#     seed_pickers: int = 505,\n",
    "#     ):\n",
    "\n",
    "#     countries = [\n",
    "#         (\"US\",\"United States\"), (\"BR\",\"Brazil\"), (\"IN\",\"India\"), (\"GB\",\"United Kingdom\"),\n",
    "#         (\"DE\",\"Germany\"), (\"FR\",\"France\"), (\"ES\",\"Spain\"), (\"CA\",\"Canada\"),\n",
    "#         (\"AU\",\"Australia\"), (\"MX\",\"Mexico\"), (\"AR\",\"Argentina\"), (\"CO\",\"Colombia\"),\n",
    "#         (\"PT\",\"Portugal\"), (\"IT\",\"Italy\"), (\"NL\",\"Netherlands\"), (\"SE\",\"Sweden\"),\n",
    "#         (\"JP\",\"Japan\"), (\"KR\",\"South Korea\"), (\"ZA\",\"South Africa\"), (\"NG\",\"Nigeria\")\n",
    "#     ]\n",
    "#     country_codes  = [c for c,_ in countries]\n",
    "#     country_weights = [10, 18, 18, 5, 4, 4, 4, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2]\n",
    "\n",
    "#     user_professions = ['Data Engineer','Data Architect','Data Analyst','Data Scientist','Developer']\n",
    "\n",
    "#     courses = [\n",
    "#         'Building a Data Lakehouse with SQL and DDP',\n",
    "#         'Mastering SQL on Databricks',\n",
    "#         'Data Heroes Mentorship Program',\n",
    "#         'Book Club - Spark the Definitive Guide',\n",
    "#         'Book Club - Delta Lake the Definitive Guide'\n",
    "#     ]\n",
    "#     course_distribution = [15, 21, 30, 19, 15]\n",
    "#     course_prices = [99.00, 297.00, 997.00, 297.00, 297.00]\n",
    "\n",
    "#     email_providers = ['@gmail.com','@hotmail.com','@outlook.com']\n",
    "#     email_distribution = [60, 25, 15]\n",
    "\n",
    "#     gender = ['M','F']\n",
    "#     gender_distribution = [49, 51]\n",
    "\n",
    "#     payment_methods = ['credit_card','debit_card','paypal','apple_pay','google_pay']\n",
    "#     payment_method_distribution = [55, 15, 12, 8, 10]\n",
    "\n",
    "#     discounts = [0.05, 0.10, 0.15]\n",
    "#     discount_distribution = [50, 35, 15]\n",
    "\n",
    "#     installments = [1, 2, 3, 10, 12]\n",
    "#     installment_distribution = [20, 15, 5, 30, 30]\n",
    "\n",
    "#     invalid_count = int(orders_n * orphan_rate)\n",
    "#     invalid_user_id = users_n + 9999\n",
    "\n",
    "#     drop_volume(chapter_number)\n",
    "#     create_volume(chapter_number)\n",
    "#     main_temp_path = f\"/Volumes/workspace/default/chapter_{chapter_number}/temp\"\n",
    "\n",
    "#     gen_users = (\n",
    "#         dg.DataGenerator(spark, name=\"users_gen\", rows=users_n, partitions=4,\n",
    "#                          randomSeedMethod=\"hash_fieldname\")\n",
    "#         .withIdOutput()\n",
    "#         .withColumn(\"user_id\", T.LongType(), expr=\"id + 1\")\n",
    "#         .withColumn(\"email_provider\", T.StringType(), values=email_providers,\n",
    "#                     weights=email_distribution, baseColumn=[\"user_id\"])\n",
    "#         .withColumn(\"email\", T.StringType(),\n",
    "#                     expr=\"concat('user_', lpad(cast(user_id as string), 6, '0'), email_provider)\",\n",
    "#                     baseColumn=[\"user_id\",\"email_provider\"])\n",
    "#         .withColumn(\"gender\", T.StringType(), values=gender,\n",
    "#                     weights=gender_distribution, baseColumn=[\"user_id\"],\n",
    "#                     nullable=True, percentNulls=0.08)\n",
    "#         .withColumn(\"profession\", T.StringType(), values=user_professions, baseColumn=[\"user_id\"],\n",
    "#                     nullable=True, percentNulls=0.05)\n",
    "#         .withColumn(\"country_code\", T.StringType(), values=country_codes,\n",
    "#                     weights=country_weights, baseColumn=[\"user_id\"])\n",
    "#     )\n",
    "#     users_df = gen_users.build().select(\"user_id\",\"email\",\"gender\",\"profession\",\"country_code\")\n",
    "#     country_map = F.create_map([F.lit(x) for kv in countries for x in kv])\n",
    "#     users_df = users_df.withColumn(\"country\", country_map[F.col(\"country_code\")]) \\\n",
    "#                        .select(\"user_id\",\"email\",\"gender\",\"profession\",\"country_code\",\"country\")\n",
    "\n",
    "#     products_df = spark.createDataFrame(\n",
    "#         list(zip(range(1, len(courses)+1), courses, course_prices)),\n",
    "#         schema=\"product_id long, product_name string, base_price double\"\n",
    "#     )\n",
    "\n",
    "#     day_offset_expr = f\"cast(pmod(abs(xxhash64(order_id, {seed_orders})), 365) as int)\"\n",
    "#     sec_of_day_expr = f\"(cast(pmod(abs(xxhash64(order_id, {seed_orders}+1)), 86399) as int) + 1)\"\n",
    "#     order_ts_expr = (\n",
    "#         f\"to_timestamp(from_unixtime(unix_timestamp('2025-01-01 00:00:00')\"\n",
    "#         f\" + {day_offset_expr} * 86400 + {sec_of_day_expr}))\"\n",
    "#     )\n",
    "\n",
    "#     gen_orders = (\n",
    "#         dg.DataGenerator(spark, name=\"orders_gen\", rows=orders_n, partitions=4,\n",
    "#                          randomSeedMethod=\"hash_fieldname\")\n",
    "#         .withIdOutput()\n",
    "#         .withColumn(\"order_id\", T.LongType(), expr=\"id + 1\")\n",
    "#         .withColumn(\"user_id\", T.LongType(), minValue=1, maxValue=users_n)\n",
    "#         .withColumn(\"order_date\", T.TimestampType(), expr=order_ts_expr)\n",
    "#         .withColumn(\"payment_method\", T.StringType(), values=payment_methods,\n",
    "#                     weights=payment_method_distribution)\n",
    "#         .withColumn(\"installments\", T.IntegerType(), values=installments,\n",
    "#                     weights=installment_distribution)\n",
    "#     )\n",
    "\n",
    "#     orders_valid = gen_orders.build().select(\n",
    "#         \"order_id\",\"user_id\",\"order_date\",\"payment_method\",\"installments\"\n",
    "#     )\n",
    "\n",
    "#     orphans_ids = (\n",
    "#         orders_valid\n",
    "#         .select(\"order_id\", F.xxhash64(\"order_id\", F.lit(seed_orders)).alias(\"h\"))\n",
    "#         .orderBy(\"h\").limit(invalid_count)\n",
    "#         .select(\"order_id\").withColumn(\"is_orphan\", F.lit(True))\n",
    "#     )\n",
    "#     orders_df = (\n",
    "#         orders_valid.join(orphans_ids, \"order_id\", \"left\")\n",
    "#                     .withColumn(\"user_id\",\n",
    "#                         F.when(F.col(\"is_orphan\"), F.lit(invalid_user_id)).otherwise(F.col(\"user_id\")).cast(\"long\"))\n",
    "#                     .drop(\"is_orphan\")\n",
    "#     )\n",
    "\n",
    "#     weights_lines = [20, 25, 30, 15, 10]\n",
    "#     cum_lines = [sum(weights_lines[:i+1]) for i in range(len(weights_lines))]\n",
    "#     bucket_lines = F.pmod(F.xxhash64(F.col(\"order_id\"), F.lit(seed_lines)), F.lit(100)).cast(\"int\")\n",
    "\n",
    "#     order_with_count = (\n",
    "#         orders_df.select(\"order_id\",\"user_id\")\n",
    "#         .withColumn(\n",
    "#             \"line_count\",\n",
    "#             F.when(bucket_lines < cum_lines[0], F.lit(1))\n",
    "#              .when(bucket_lines < cum_lines[1], F.lit(2))\n",
    "#              .when(bucket_lines < cum_lines[2], F.lit(3))\n",
    "#              .when(bucket_lines < cum_lines[3], F.lit(4))\n",
    "#              .otherwise(F.lit(5))\n",
    "#         )\n",
    "#     )\n",
    "#     order_lines = (\n",
    "#         order_with_count\n",
    "#         .withColumn(\"line_no\", F.explode(F.sequence(F.lit(1), F.col(\"line_count\"))))\n",
    "#         .drop(\"line_count\")\n",
    "#     )\n",
    "\n",
    "#     w_user = Window.partitionBy(\"user_id\").orderBy(F.col(\"order_id\").asc(), F.col(\"line_no\").asc())\n",
    "#     order_lines_ranked = order_lines.withColumn(\"user_line_rank\", F.row_number().over(w_user)) \\\n",
    "#                                     .filter(F.col(\"user_line_rank\") <= 5)\n",
    "\n",
    "#     h = [F.xxhash64(F.col(\"user_id\"), F.lit(i), F.lit(seed_pickers)) for i in range(1, 6)]\n",
    "#     perm_array = F.array_sort(F.array(\n",
    "#         F.struct(h[0].alias(\"h\"), F.lit(1).alias(\"pid\")),\n",
    "#         F.struct(h[1].alias(\"h\"), F.lit(2).alias(\"pid\")),\n",
    "#         F.struct(h[2].alias(\"h\"), F.lit(3).alias(\"pid\")),\n",
    "#         F.struct(h[3].alias(\"h\"), F.lit(4).alias(\"pid\")),\n",
    "#         F.struct(h[4].alias(\"h\"), F.lit(5).alias(\"pid\"))\n",
    "#     ))\n",
    "#     order_lines_with_product = (\n",
    "#         order_lines_ranked\n",
    "#         .withColumn(\"product_struct\", F.element_at(perm_array, F.col(\"user_line_rank\")))\n",
    "#         .withColumn(\"product_id\", F.col(\"product_struct.pid\").cast(\"long\"))\n",
    "#         .drop(\"product_struct\")\n",
    "#     )\n",
    "\n",
    "#     cum_disc = [sum(discount_distribution[:i+1]) for i in range(len(discount_distribution))]\n",
    "#     bucket_disc = F.pmod(F.xxhash64(F.col(\"order_id\"), F.col(\"line_no\"), F.lit(seed_details+1)), F.lit(100)).cast(\"int\")\n",
    "#     discount_col = (\n",
    "#         F.when(bucket_disc < cum_disc[0], F.lit(discounts[0]))\n",
    "#          .when(bucket_disc < cum_disc[1], F.lit(discounts[1]))\n",
    "#          .otherwise(F.lit(discounts[2]))\n",
    "#     )\n",
    "#     details_df = (\n",
    "#         order_lines_with_product\n",
    "#         .withColumn(\"discount\", discount_col.cast(\"double\"))\n",
    "#         .join(products_df.select(\"product_id\",\"base_price\"), \"product_id\", \"inner\")\n",
    "#         .withColumn(\"unit_price\", F.col(\"base_price\"))\n",
    "#         .select(\"order_id\",\"product_id\",\"unit_price\",\"discount\")\n",
    "#     )\n",
    "\n",
    "#     write_all_formats(users_df,    main_temp_path, \"users\",         one_file=False)\n",
    "#     write_all_formats(products_df, main_temp_path, \"products\",      one_file=True)\n",
    "#     write_all_formats(orders_df,   main_temp_path, \"orders\",        one_file=False)\n",
    "#     write_all_formats(details_df,  main_temp_path, \"order_details\", one_file=False)\n",
    "\n",
    "#     create_complex_data_files(chapter_number, main_temp_path)\n",
    "#     create_enumerated_files(f\"{main_temp_path}/array_products_by_order/json/\", \"array_products_by_order\", \"json\")\n",
    "#     create_enumerated_files(f\"{main_temp_path}/order_details_dict/json/\",       \"order_details_dict\",       \"json\")\n",
    "\n",
    "#     dbutils.fs.rm(main_temp_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb7ace4-bd15-44e2-8140-a64dbb79b918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbldatagen\n",
    "\n",
    "import dbldatagen as dg\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def drop_volume(chapter_number: str):\n",
    "    spark.sql(f\"drop volume if exists workspace.default.chapter_{chapter_number}\")\n",
    "\n",
    "def create_volume(chapter_number: str):\n",
    "    spark.sql(f\"\"\"\n",
    "        create volume if not exists workspace.default.chapter_{chapter_number}\n",
    "        comment 'Datasets for chapter {chapter_number}'\n",
    "    \"\"\")\n",
    "\n",
    "def create_enumerated_files(temp_path: str, table_name: str, file_format: str):\n",
    "    data_files = [f for f in dbutils.fs.ls(temp_path) if f.name.startswith(\"part-\")]\n",
    "    for index, file_name in enumerate(data_files, start=1):\n",
    "        path = temp_path.replace('temp/', '')\n",
    "        dbutils.fs.mv(file_name.path, f\"{path}{table_name}_0{index}.{file_format}\")\n",
    "\n",
    "def create_complex_data_files(chapter_number: str, temp_root: str):\n",
    "    od_path = f\"/Volumes/workspace/default/chapter_{chapter_number}/order_details/parquet/\"\n",
    "    df = spark.read.parquet(od_path)\n",
    "\n",
    "    array_products = (\n",
    "        df.groupBy(\"order_id\")\n",
    "          .agg(F.collect_list(\"product_id\").alias(\"array_products\"))\n",
    "    )\n",
    "\n",
    "    order_details_dict = (\n",
    "        df.groupBy(\"order_id\")\n",
    "          .agg(F.collect_list(F.struct(\n",
    "                F.col(\"product_id\").alias(\"productid\"),\n",
    "                F.col(\"unit_price\").alias(\"unitprice\"),\n",
    "                F.col(\"discount\").alias(\"discount\")\n",
    "          )).alias(\"order_details\"))\n",
    "    )\n",
    "\n",
    "    (array_products.repartition(4)\n",
    "        .write.mode(\"overwrite\").format(\"json\")\n",
    "        .save(f\"{temp_root}/array_products_by_order/json/\"))\n",
    "\n",
    "    (order_details_dict.repartition(4)\n",
    "        .write.mode(\"overwrite\").format(\"json\")\n",
    "        .save(f\"{temp_root}/order_details_dict/json/\"))\n",
    "\n",
    "def write_all_formats(df, main_temp_path: str, table_name: str, one_file: bool = False):\n",
    "    dict_types = { \n",
    "        \"csv\":     {\"options\": {\"delimiter\": \",\", \"header\": \"true\"}},\n",
    "        \"parquet\": {\"options\": {}},\n",
    "        \"json\":    {\"options\": {}},\n",
    "        \"avro\":    {\"options\": {}},\n",
    "        \"orc\":     {\"options\": {}}\n",
    "    }\n",
    "    for fmt, spec in dict_types.items():\n",
    "        tmp = f\"{main_temp_path}/{table_name}/{fmt}/\"\n",
    "        writer_df = df.coalesce(1) if one_file else df.repartition(4)\n",
    "        (writer_df.write\n",
    "            .mode(\"overwrite\")\n",
    "            .options(**spec[\"options\"])\n",
    "            .format(fmt)\n",
    "            .save(tmp))\n",
    "        create_enumerated_files(tmp, table_name, fmt)\n",
    "\n",
    "def generate_and_write_to_volume(\n",
    "    chapter_number: str,\n",
    "    users_n: int = 1000,\n",
    "    orders_n: int = 500,\n",
    "    orphan_rate: float = 0.01,\n",
    "    seed_users: int = 101,\n",
    "    seed_orders: int = 202,\n",
    "    seed_details: int = 303,\n",
    "    seed_lines: int = 404,\n",
    "    seed_pickers: int = 505,\n",
    "):\n",
    "\n",
    "    countries = [\n",
    "        (\"US\",\"United States\"), (\"BR\",\"Brazil\"), (\"IN\",\"India\"), (\"GB\",\"United Kingdom\"),\n",
    "        (\"DE\",\"Germany\"), (\"FR\",\"France\"), (\"ES\",\"Spain\"), (\"CA\",\"Canada\"),\n",
    "        (\"AU\",\"Australia\"), (\"MX\",\"Mexico\"), (\"AR\",\"Argentina\"), (\"CO\",\"Colombia\"),\n",
    "        (\"PT\",\"Portugal\"), (\"IT\",\"Italy\"), (\"NL\",\"Netherlands\"), (\"SE\",\"Sweden\"),\n",
    "        (\"JP\",\"Japan\"), (\"KR\",\"South Korea\"), (\"ZA\",\"South Africa\"), (\"NG\",\"Nigeria\")\n",
    "    ]\n",
    "    country_codes  = [c for c,_ in countries]\n",
    "    country_weights = [10, 18, 18, 5, 4, 4, 4, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2]\n",
    "\n",
    "    user_professions = ['Data Engineer','Data Architect','Data Analyst','Data Scientist','Developer']\n",
    "    courses = [\n",
    "        'Building a Data Lakehouse with SQL and DDP',\n",
    "        'Mastering SQL on Databricks',\n",
    "        'Data Heroes Mentorship Program',\n",
    "        'Book Club - Spark the Definitive Guide',\n",
    "        'Book Club - Delta Lake the Definitive Guide'\n",
    "    ]\n",
    "    course_prices = [99.00, 297.00, 997.00, 297.00, 297.00]\n",
    "\n",
    "    email_providers = ['@gmail.com','@hotmail.com','@outlook.com']\n",
    "    email_distribution = [60, 25, 15]\n",
    "    gender = ['M','F']\n",
    "    gender_distribution = [49, 51]\n",
    "    payment_methods = ['credit_card','debit_card','paypal','apple_pay','google_pay']\n",
    "    payment_method_distribution = [55, 15, 12, 8, 10]\n",
    "    discounts = [0.05, 0.10, 0.15]\n",
    "    discount_distribution = [50, 35, 15]\n",
    "    installments = [1, 2, 3, 10, 12]\n",
    "    installment_distribution = [20, 15, 5, 30, 30]\n",
    "\n",
    "    invalid_count = int(orders_n * orphan_rate)\n",
    "    invalid_user_id = users_n + 9999\n",
    "\n",
    "    drop_volume(chapter_number)\n",
    "    create_volume(chapter_number)\n",
    "    main_temp_path = f\"/Volumes/workspace/default/chapter_{chapter_number}/temp\"\n",
    "\n",
    "    # USERS — garantir ordem com baseColumn\n",
    "    gen_users = (\n",
    "        dg.DataGenerator(spark, name=\"users_gen\", rows=users_n, partitions=4, randomSeedMethod=\"hash_fieldname\")\n",
    "        .withIdOutput()\n",
    "        .withColumn(\"user_id\", T.LongType(), expr=\"id + 1\")\n",
    "        .withColumn(\"email_provider\", T.StringType(), values=email_providers, weights=email_distribution, baseColumn=[\"user_id\"])\n",
    "        .withColumn(\n",
    "            \"email\", T.StringType(),\n",
    "            expr=\"concat('user_', lpad(cast(user_id as string), 6, '0'), email_provider)\",\n",
    "            baseColumn=[\"user_id\", \"email_provider\"]\n",
    "        )\n",
    "        .withColumn(\"gender\", T.StringType(), values=gender, weights=gender_distribution, baseColumn=[\"user_id\"],\n",
    "                    nullable=True, percentNulls=0.08)\n",
    "        .withColumn(\"profession\", T.StringType(), values=user_professions, baseColumn=[\"user_id\"],\n",
    "                    nullable=True, percentNulls=0.05)\n",
    "        .withColumn(\"country_code\", T.StringType(), values=country_codes, weights=country_weights, baseColumn=[\"user_id\"])\n",
    "    )\n",
    "    users_df = gen_users.build().select(\"user_id\",\"email\",\"gender\",\"profession\",\"country_code\")\n",
    "    country_map = F.create_map([F.lit(x) for kv in countries for x in kv])\n",
    "    users_df = users_df.withColumn(\"country\", country_map[F.col(\"country_code\")]) \\\n",
    "                       .select(\"user_id\",\"email\",\"gender\",\"profession\",\"country_code\",\"country\")\n",
    "\n",
    "    # PRODUCTS\n",
    "    products_df = spark.createDataFrame(\n",
    "        list(zip(range(1, len(courses)+1), courses, course_prices)),\n",
    "        schema=\"product_id long, product_name string, base_price double\"\n",
    "    )\n",
    "\n",
    "    # ORDERS — order_id e user_id com seeds diferentes + timestamp não-meia-noite\n",
    "    day_offset_expr = f\"cast(pmod(abs(xxhash64(id + 1, {seed_orders})), 365) as int)\"\n",
    "    sec_of_day_expr = f\"(cast(pmod(abs(xxhash64(id + 1, {seed_orders}+1)), 86399) as int) + 1)\"\n",
    "    order_ts_expr = (\n",
    "        f\"to_timestamp(from_unixtime(unix_timestamp('2025-01-01 00:00:00')\"\n",
    "        f\" + {day_offset_expr} * 86400 + {sec_of_day_expr}))\"\n",
    "    )\n",
    "\n",
    "    gen_orders = (\n",
    "        dg.DataGenerator(spark, name=\"orders_gen\", rows=orders_n, partitions=4, randomSeedMethod=\"hash_fieldname\")\n",
    "        .withIdOutput()\n",
    "        .withColumn(\"order_id\", T.LongType(), expr=\"id + 1\")\n",
    "        .withColumn(\"user_id\", T.LongType(), expr=f\"pmod(abs(xxhash64(id + 1, {seed_users})), {users_n}) + 1\")\n",
    "        .withColumn(\"order_date\", T.TimestampType(), expr=order_ts_expr)\n",
    "        .withColumn(\"payment_method\", T.StringType(), values=payment_methods, weights=payment_method_distribution)\n",
    "        # valor preliminar de installments (será substituído por regra realista)\n",
    "        .withColumn(\"installments\", T.IntegerType(), values=installments, weights=installment_distribution)\n",
    "    )\n",
    "\n",
    "    # Build orders e aplicar regra realista de parcelamento com base em payment_method + país do usuário\n",
    "    orders_valid = gen_orders.build().select(\"order_id\",\"user_id\",\"order_date\",\"payment_method\",\"installments\")\n",
    "\n",
    "    # Trazer country_code do usuário para aplicar regra\n",
    "    orders_with_country = (\n",
    "        orders_valid.join(users_df.select(\"user_id\",\"country_code\"), \"user_id\", \"left\")\n",
    "    )\n",
    "\n",
    "    # Hashes determinísticos para decidir se parcela e quantas parcelas (2–12)\n",
    "    decide_parcelar = F.pmod(F.abs(F.xxhash64(F.col(\"order_id\"), F.lit(seed_orders + 7))), F.lit(100)).cast(\"int\")\n",
    "    parcelas_hash   = F.pmod(F.abs(F.xxhash64(F.col(\"order_id\"), F.lit(seed_orders + 8))), F.lit(11)).cast(\"int\")  # 0..10\n",
    "\n",
    "    # países que suportam parcelamento típico via cartão\n",
    "    latam_countries = F.array(F.lit(\"BR\"), F.lit(\"MX\"), F.lit(\"AR\"))\n",
    "\n",
    "    installments_realistic = (\n",
    "        F.when(\n",
    "            (F.col(\"payment_method\") == \"credit_card\") &\n",
    "            (F.array_contains(latam_countries, F.col(\"country_code\"))),\n",
    "            F.when(decide_parcelar < 30, F.lit(1))  # ~30% 1x\n",
    "             .otherwise(parcelas_hash + 2)          # 2..12\n",
    "        )\n",
    "        .otherwise(F.lit(1))  # PayPal, débito, Apple Pay, Google Pay e demais países: 1x\n",
    "    )\n",
    "\n",
    "    orders_df = (\n",
    "        orders_with_country\n",
    "        .withColumn(\"installments\", installments_realistic.cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # ORPHANS (injeta user_id inválido em fração dos pedidos)\n",
    "    orphans_ids = (\n",
    "        orders_df.select(\"order_id\", F.xxhash64(\"order_id\", F.lit(seed_orders)).alias(\"h\"))\n",
    "                 .orderBy(\"h\").limit(invalid_count)\n",
    "                 .select(\"order_id\").withColumn(\"is_orphan\", F.lit(True))\n",
    "    )\n",
    "\n",
    "    orders_df = (\n",
    "        orders_df.join(orphans_ids, \"order_id\", \"left\")\n",
    "                 .withColumn(\"user_id\",\n",
    "                     F.when(F.col(\"is_orphan\"), F.lit(invalid_user_id)).otherwise(F.col(\"user_id\")).cast(\"long\"))\n",
    "                 .drop(\"is_orphan\", \"country_code\")  # country_code não precisa ir para a tabela final\n",
    "    )\n",
    "\n",
    "    # ORDER DETAILS\n",
    "    weights_lines = [20, 25, 30, 15, 10]\n",
    "    cum_lines = [sum(weights_lines[:i+1]) for i in range(len(weights_lines))]\n",
    "    bucket_lines = F.pmod(F.xxhash64(F.col(\"order_id\"), F.lit(seed_lines)), F.lit(100)).cast(\"int\")\n",
    "\n",
    "    order_with_count = (\n",
    "        orders_df.select(\"order_id\",\"user_id\")\n",
    "        .withColumn(\"line_count\",\n",
    "            F.when(bucket_lines < cum_lines[0], F.lit(1))\n",
    "             .when(bucket_lines < cum_lines[1], F.lit(2))\n",
    "             .when(bucket_lines < cum_lines[2], F.lit(3))\n",
    "             .when(bucket_lines < cum_lines[3], F.lit(4))\n",
    "             .otherwise(F.lit(5)))\n",
    "    )\n",
    "    order_lines = order_with_count.withColumn(\"line_no\", F.explode(F.sequence(F.lit(1), F.col(\"line_count\")))).drop(\"line_count\")\n",
    "\n",
    "    w_user = Window.partitionBy(\"user_id\").orderBy(F.col(\"order_id\").asc(), F.col(\"line_no\").asc())\n",
    "    order_lines_ranked = order_lines.withColumn(\"user_line_rank\", F.row_number().over(w_user)).filter(F.col(\"user_line_rank\") <= 5)\n",
    "\n",
    "    h = [F.xxhash64(F.col(\"user_id\"), F.lit(i), F.lit(seed_pickers)) for i in range(1, 6)]\n",
    "    perm_array = F.array_sort(F.array(\n",
    "        F.struct(h[0].alias(\"h\"), F.lit(1).alias(\"pid\")),\n",
    "        F.struct(h[1].alias(\"h\"), F.lit(2).alias(\"pid\")),\n",
    "        F.struct(h[2].alias(\"h\"), F.lit(3).alias(\"pid\")),\n",
    "        F.struct(h[3].alias(\"h\"), F.lit(4).alias(\"pid\")),\n",
    "        F.struct(h[4].alias(\"h\"), F.lit(5).alias(\"pid\"))\n",
    "    ))\n",
    "\n",
    "    order_lines_with_product = (\n",
    "        order_lines_ranked\n",
    "        .withColumn(\"product_struct\", F.element_at(perm_array, F.col(\"user_line_rank\")))\n",
    "        .withColumn(\"product_id\", F.col(\"product_struct.pid\").cast(\"long\"))\n",
    "        .drop(\"product_struct\")\n",
    "    )\n",
    "\n",
    "    cum_disc = [sum(discount_distribution[:i+1]) for i in range(len(discount_distribution))]\n",
    "    bucket_disc = F.pmod(F.xxhash64(F.col(\"order_id\"), F.col(\"line_no\"), F.lit(seed_details+1)), F.lit(100)).cast(\"int\")\n",
    "    discount_col = (\n",
    "        F.when(bucket_disc < cum_disc[0], F.lit(discounts[0]))\n",
    "         .when(bucket_disc < cum_disc[1], F.lit(discounts[1]))\n",
    "         .otherwise(F.lit(discounts[2]))\n",
    "    )\n",
    "\n",
    "    details_df = (\n",
    "        order_lines_with_product\n",
    "        .withColumn(\"discount\", discount_col.cast(\"double\"))\n",
    "        .join(products_df.select(\"product_id\",\"base_price\"), \"product_id\", \"inner\")\n",
    "        .join(orders_df.select(\"order_id\",\"installments\"), \"order_id\", \"inner\")\n",
    "        .withColumn(\"unit_price\", F.round((F.col(\"base_price\") * (1 - F.col(\"discount\"))) / F.col(\"installments\"), 2))\n",
    "        .select(\"order_id\",\"product_id\",\"unit_price\",\"discount\")\n",
    "    )\n",
    "\n",
    "    # WRITE\n",
    "    write_all_formats(users_df, main_temp_path, \"users\")\n",
    "    write_all_formats(products_df, main_temp_path, \"products\", one_file=True)\n",
    "    write_all_formats(orders_df.select(\"order_id\",\"user_id\",\"order_date\",\"payment_method\",\"installments\"),\n",
    "                      main_temp_path, \"orders\")\n",
    "    write_all_formats(details_df,  main_temp_path, \"order_details\")\n",
    "\n",
    "    # DERIVADOS COMPLEXOS\n",
    "    create_complex_data_files(chapter_number, main_temp_path)\n",
    "    create_enumerated_files(f\"{main_temp_path}/array_products_by_order/json/\", \"array_products_by_order\", \"json\")\n",
    "    create_enumerated_files(f\"{main_temp_path}/order_details_dict/json/\", \"order_details_dict\", \"json\")\n",
    "\n",
    "    # CLEANUP TEMP\n",
    "    dbutils.fs.rm(main_temp_path, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4756902e-39cf-4648-b53c-b3204648ba7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_and_write_to_volume(chapter_number=\"05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e3e0c2-76f8-4bf4-8a3b-3af327b2b137",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760379151356}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(spark.read.parquet('/Volumes/workspace/default/chapter_05/orders/parquet'))\n",
    "# display(spark.read.parquet('/Volumes/workspace/default/chapter_05/users/parquet'))\n",
    "# display(spark.read.parquet('/Volumes/workspace/default/chapter_05/order_details/parquet'))\n",
    "# display(spark.read.parquet('/Volumes/workspace/default/chapter_05/products/parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302a59d8-3c40-4a32-86f9-c65e9e265c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet('/Volumes/workspace/default/chapter_05/orders/parquet').createOrReplaceTempView('orders')\n",
    "spark.read.parquet('/Volumes/workspace/default/chapter_05/order_details/parquet').createOrReplaceTempView('order_details')\n",
    "spark.read.parquet('/Volumes/workspace/default/chapter_05/users/parquet').createOrReplaceTempView('users')\n",
    "spark.read.parquet('/Volumes/workspace/default/chapter_05/products/parquet').createOrReplaceTempView('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7adadccd-e368-4954-9692-121c6e527de7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760380337751}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "\n",
    "SELECT od.*\n",
    "FROM orders as o \n",
    "join order_details as od on o.order_id = od.order_id\n",
    "join users as u on o.user_id = u.user_id\n",
    "join products as p on p.product_id = od.product_id\n",
    "order by o.order_date"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6031849561206878,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "setup_chapter_05",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
