{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b419b14b-1084-4903-a54e-a550afe919ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_volume(chapter_number: str):\n",
    "    spark.sql(f\"drop volume if exists workspace.default.chapter_{chapter_number}\")\n",
    "\n",
    "def create_volume(chapter_number: str):\n",
    "    spark.sql(f\"\"\"\n",
    "        create volume if not exists workspace.default.chapter_{chapter_number}\n",
    "        comment 'Datasets for chapter {chapter_number}'\n",
    "    \"\"\")\n",
    "\n",
    "def create_enumerated_files(temp_path: str, table_name: str, file_format: str):\n",
    "    data_files = [f for f in dbutils.fs.ls(temp_path) if f.name.startswith(\"part-\")]\n",
    "    for index, file_name in enumerate(data_files, start=1):\n",
    "        path = temp_path.replace('temp/', '')\n",
    "        dbutils.fs.mv(file_name.path, f\"{path}{table_name}_0{index}.{file_format}\")\n",
    "\n",
    "def create_complex_data_files(chapter_number: str, temp_path: str):\n",
    "    \n",
    "    df = spark.read.parquet(\n",
    "        f'/Volumes/workspace/default/chapter_{chapter_number}/orderdetails/parquet/'\n",
    "    )\n",
    "    df.createOrReplaceTempView('orderdetails_source')\n",
    "    \n",
    "    array_products = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            orderid, \n",
    "            ARRAY_AGG(productid) AS array_products\n",
    "        FROM orderdetails_source\n",
    "        GROUP BY orderid;\n",
    "    \"\"\")\n",
    "\n",
    "    array_products.repartition(4) \\\n",
    "      .write \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .format('parquet') \\\n",
    "      .save(f'{temp_path}/array_products_by_order/parquet/')\n",
    "    \n",
    "    order_details_dict = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            orderid, \n",
    "            ARRAY_AGG(\n",
    "                NAMED_STRUCT(\n",
    "                    'productid', productid, \n",
    "                    'unitprice', unitprice, \n",
    "                    'qty', qty, \n",
    "                    'discount', discount)\n",
    "                ) AS order_details\n",
    "        FROM orderdetails_source\n",
    "        GROUP BY orderid\n",
    "    \"\"\")\n",
    "\n",
    "    order_details_dict.repartition(4) \\\n",
    "                      .write \\\n",
    "                      .mode(\"overwrite\") \\\n",
    "                      .format('json') \\\n",
    "                      .save(f'{temp_path}/order_details_dict/parquet/')\n",
    "\n",
    "def get_sample_files(chapter_number: str):\n",
    "    import pandas as pd\n",
    "\n",
    "    drop_volume(chapter_number)\n",
    "    create_volume(chapter_number)\n",
    "\n",
    "    tables_list = ['Employees','Suppliers','Categories','Products','Customers','Shippers','Orders','OrderDetails','Tests','Scores','Nums']\n",
    "\n",
    "    dict_types = {\n",
    "        \"csv\": {\"options\": {\"delimiter\": \",\",\"header\": \"true\"}},\n",
    "        \"parquet\": {\"options\": {}},\n",
    "        \"json\": {\"options\": {}},\n",
    "        \"avro\": {\"options\": {}},\n",
    "        \"orc\": {\"options\": {}}\n",
    "    }\n",
    "\n",
    "    main_temp_path = f'/Volumes/workspace/default/chapter_{chapter_number}/temp'\n",
    "\n",
    "    for table in tables_list:\n",
    "        parquet_file = f'https://raw.githubusercontent.com/ajluz/tsql_database/main/{table}.parquet'\n",
    "        df = pd.read_parquet(parquet_file, engine='auto')\n",
    "        sparkdf = spark.createDataFrame(df)\n",
    "        table_temp_path = f'{main_temp_path}/{table.lower()}'\n",
    "\n",
    "        for fmt, spec in dict_types.items():\n",
    "            options = spec[\"options\"]\n",
    "            type_temp_path = f'{table_temp_path}/{fmt}/'\n",
    "            \n",
    "            sparkdf.repartition(4) \\\n",
    "                   .write \\\n",
    "                   .mode(\"overwrite\") \\\n",
    "                   .options(**options) \\\n",
    "                   .format(fmt) \\\n",
    "                   .save(type_temp_path)\n",
    "\n",
    "            create_enumerated_files(type_temp_path, table.lower(), fmt)\n",
    "        \n",
    "        if table == 'OrderDetails':\n",
    "            create_complex_data_files(chapter_number, main_temp_path)\n",
    "            create_enumerated_files(\n",
    "                f'{main_temp_path}/array_products_by_order/parquet/',\n",
    "                'array_products_by_order',\n",
    "                'parquet'\n",
    "            )\n",
    "            create_enumerated_files(\n",
    "                f'{main_temp_path}/order_details_dict/parquet/',\n",
    "                'order_details_dict',\n",
    "                'parquet'\n",
    "            )\n",
    "        \n",
    "    dbutils.fs.rm(main_temp_path, True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8455174265418319,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "helper_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
